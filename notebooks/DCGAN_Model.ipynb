{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b298f2-0de6-4e78-bcb7-99df40867c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "DATASET_ROOT = \"data/HQ-50K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c25473-8767-432a-8f90-cf8d273f692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images shape: (38506, 64, 64, 3)\n",
      "Train: (34655, 64, 64, 3) Test: (3851, 64, 64, 3)\n",
      "Train: (34655, 64, 64, 3) Test: (3851, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_and_crop_images(folder, img_size=64):\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\")\n",
    "    images = []\n",
    "\n",
    "    for path in glob.iglob(os.path.join(folder, \"**\", \"*\"), recursive=True):\n",
    "        if path.lower().endswith(exts):\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                img = img.resize((img_size, img_size))\n",
    "                images.append(np.array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {path}: {e}\")\n",
    "\n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    images = (images - 127.5) / 127.5  #normalize to [-1, 1]\n",
    "    return images\n",
    "\n",
    "all_images = load_and_crop_images(DATASET_ROOT)\n",
    "print(\"Loaded images shape:\", all_images.shape)\n",
    "\n",
    "#separate into train and test at 0.1 ratio\n",
    "train_images, test_images = train_test_split(all_images, test_size=0.1, random_state=42)\n",
    "print(\"Train:\", train_images.shape, \"Test:\", test_images.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "#add Gaussian blur to clean images to create pairs\n",
    "dataset = dataset.map(lambda x: (tf.clip_by_value(x + tf.random.normal(tf.shape(x), stddev=0.1), -1.0, 1.0), x))\n",
    "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "print(\"Train:\", train_images.shape, \"Test:\", test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "766c4913-1d22-484b-a3d0-c4272480e446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(63.5), np.float64(63.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_generator_model():\n",
    "    inputs = tf.keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "    #downsample\n",
    "    x = layers.Conv2D(64, (5, 5), strides=2, padding=\"same\")(inputs)   # 32x32\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(128, (5, 5), strides=2, padding=\"same\")(x)      # 16x16\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    #upsample\n",
    "    x = layers.Conv2DTranspose(128, (5, 5), strides=2, padding=\"same\")(x)  # 32x32\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=2, padding=\"same\")(x)   # 64x64\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    outputs = layers.Conv2D(3, (5, 5), activation=\"tanh\", padding=\"same\")(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "sample_noisy = train_images[0:1]\n",
    "generated_image = generator(sample_noisy, training=False)\n",
    "\n",
    "plt.imshow((generated_image[0] * 0.5 + 0.5))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999c1e23-8da3-46aa-b934-fdb31cd50c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.02010058]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[-0.02010058]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def make_discriminator_model():\n",
    "    noisy_input = tf.keras.Input(shape=(64, 64, 3))\n",
    "    clean_input = tf.keras.Input(shape=(64, 64, 3))\n",
    "\n",
    "    x = tf.keras.layers.Concatenate()([noisy_input, clean_input])\n",
    "\n",
    "    #reduce down to 1 layer\n",
    "    x = layers.Conv2D(64, (5, 5), strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(128, (5, 5), strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Conv2D(256, (5, 5), strides=2, padding=\"same\")(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    return tf.keras.Model([noisy_input, clean_input], x)\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "sample_noisy_tensor = tf.convert_to_tensor(sample_noisy, dtype=tf.float32)\n",
    "decision = discriminator([sample_noisy_tensor, generated_image])\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "423f4779-8171-4f7d-92f9-657b9a6a8d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizers complete.\n"
     ]
    }
   ],
   "source": [
    "#calculate cross entropy for loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "LAMBDA = 100 #use lambda to control importance of L1\n",
    "\n",
    "def generator_loss(fake_output, real_image, generated_image):\n",
    "    adv_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(real_image - generated_image))\n",
    "    return adv_loss + LAMBDA * l1_loss\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "print(\"Optimizers complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073b93af-40fa-420f-bb40-7c50da94230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761795152.788864   43885 meta_optimizer.cc:967] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'gradient_tape/functional_4_3/leaky_re_lu_16_1/LeakyRelu/LeakyReluGrad_1' exist for missing node 'functional_4_3/conv2d_14_1/BiasAdd'.\n",
      "E0000 00:00:1761795612.482183   43885 meta_optimizer.cc:967] remapper failed: INVALID_ARGUMENT: Mutation::Apply error: fanout 'gradient_tape/functional_4_1/leaky_re_lu_15_1/LeakyRelu/LeakyReluGrad' exist for missing node 'functional_4_1/conv2d_13_1/BiasAdd'.\n",
      "2025-10-29 23:40:12.955648: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m noisy_batch, clean_batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# visualize progress every 5 epochs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    866\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    868\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    875\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/498/COEN691-project/venv/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#50 is destroying my laptop since I have no GPU, trying 20\n",
    "EPOCHS = 50\n",
    "\n",
    "@tf.function\n",
    "def train_step(noisy_batch, clean_batch):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noisy_batch, training=True)\n",
    "\n",
    "        real_output = discriminator([noisy_batch, clean_batch], training=True)\n",
    "        fake_output = discriminator([noisy_batch, generated_images], training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, clean_batch, generated_images)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for noisy_batch, clean_batch in dataset:\n",
    "        train_step(noisy_batch, clean_batch)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} complete\")\n",
    "    \n",
    "    #image every 5 gens to see progress\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        generate_and_save_images(generator, epoch+1, sample_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f942bb-d734-4891-bcf8-78d43df10f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  #inference mode\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow((predictions[i] * 0.5 + 0.5))\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
