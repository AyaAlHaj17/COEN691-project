{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DupAmRzuzpTC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/data/HQ-50K\"\n",
        "processed_images = np.load(\"/content/drive/MyDrive/data/data_processed/images_64.npy\")\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/GAN_results\"\n",
        "\n",
        "psnr_values = []\n",
        "train_gen_losses, train_disc_losses = [], []\n",
        "val_gen_losses, val_disc_losses = [], []\n",
        "\n",
        "#Separate 64x64 images into 90% training and 10% testing with a random seed.\n",
        "train_images, test_images = train_test_split(processed_images, test_size=0.1, random_state=42)\n",
        "\n",
        "#Grab a sample for comparisons\n",
        "sample_noisy = train_images[0:16]\n",
        "sample_clean = test_images[0:16]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "\n",
        "\n",
        "#Normalize the dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "dataset = dataset.map(lambda x: (tf.clip_by_value(x + tf.random.normal(tf.shape(x), stddev=0.1),\n",
        "                                                 -1.0, 1.0), x))\n",
        "dataset = dataset.shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "# Modify generator to accept 64x64 images instead of working up from random noise\n",
        "def make_generator_model():\n",
        "    inputs = tf.keras.Input(shape=(64, 64, 3))\n",
        "\n",
        "    x = layers.Conv2D(64, 5, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    outputs = layers.Conv2D(3, 5, activation=\"tanh\", padding=\"same\")(x)\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "#Accept a noisy image and its clean counterpart to compare against.\n",
        "def make_discriminator_model():\n",
        "    noisy_input = tf.keras.Input(shape=(64, 64, 3))\n",
        "    clean_input = tf.keras.Input(shape=(64, 64, 3))\n",
        "    x = layers.Concatenate()([noisy_input, clean_input])\n",
        "\n",
        "    x = layers.Conv2D(64, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(128, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(256, 5, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1)(x)\n",
        "\n",
        "    return tf.keras.Model([noisy_input, clean_input], x)\n",
        "\n",
        "\n",
        "#DCGAN recommends binary cross entropy to punish false positives and false negatives.\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return (cross_entropy(tf.ones_like(real_output), real_output) +\n",
        "            cross_entropy(tf.zeros_like(fake_output), fake_output))\n",
        "\n",
        "def make_generator_loss(LAMBDA):\n",
        "    def generator_loss(fake_output, real_image, generated_image):\n",
        "        adv = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "        l1 = tf.reduce_mean(tf.abs(real_image - generated_image))\n",
        "        return adv + LAMBDA * l1\n",
        "    return generator_loss\n",
        "\n",
        "\n",
        "def evaluate_psnr(generator, clean_images):\n",
        "    noisy_images = tf.clip_by_value(clean_images +\n",
        "                                    tf.random.normal(tf.shape(clean_images), stddev=0.1),\n",
        "                                    -1.0, 1.0)\n",
        "\n",
        "    generated = generator(noisy_images, training=False).numpy()\n",
        "\n",
        "    clean_rescaled = (clean_images + 1) / 2\n",
        "    gen_rescaled = (generated + 1) / 2\n",
        "\n",
        "    scores = []\n",
        "    for clean, gen in zip(clean_rescaled, gen_rescaled):\n",
        "        scores.append(compare_psnr(clean, gen, data_range=1.0))\n",
        "\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(noisy_batch, clean_batch,\n",
        "               generator, discriminator,\n",
        "               generator_optimizer, discriminator_optimizer,\n",
        "               generator_loss_fn):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noisy_batch, training=True)\n",
        "\n",
        "        real_output = discriminator([noisy_batch, clean_batch], training=True)\n",
        "        fake_output = discriminator([noisy_batch, generated_images], training=True)\n",
        "\n",
        "        gen_loss = generator_loss_fn(fake_output, clean_batch, generated_images)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    #Generate gradients reflecting the losses of each model, use to teach the model\n",
        "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    #Apply the gradients to the models\n",
        "    generator_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
        "\n",
        "#Sample the loss by using the model at each epoch and saving it to track progress\n",
        "def compute_losses(dataset, generator, discriminator, generator_loss_fn):\n",
        "    gen_losses = []\n",
        "    disc_losses = []\n",
        "\n",
        "    for noisy_batch, clean_batch in dataset:\n",
        "        generated_images = generator(noisy_batch, training=False)\n",
        "        fake_output = discriminator([noisy_batch, generated_images], training=False)\n",
        "        real_output = discriminator([noisy_batch, clean_batch], training=False)\n",
        "\n",
        "        gen_loss = generator_loss_fn(fake_output, clean_batch, generated_images)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gen_losses.append(gen_loss.numpy())\n",
        "        disc_losses.append(disc_loss.numpy())\n",
        "\n",
        "    avg_gen_loss = sum(gen_losses) / len(gen_losses)\n",
        "    avg_disc_loss = sum(disc_losses) / len(disc_losses)\n",
        "    return avg_gen_loss, avg_disc_loss\n",
        "\n",
        "def save_loss_curve(train_gen_losses, train_disc_losses,\n",
        "                    val_gen_losses, val_disc_losses,\n",
        "                    epoch, outdir):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(range(1, epoch+1), train_gen_losses, label=\"Train Gen Loss\", marker='o')\n",
        "    plt.plot(range(1, epoch+1), train_disc_losses, label=\"Train Disc Loss\", marker='o')\n",
        "    plt.plot(range(1, epoch+1), val_gen_losses, label=\"Val Gen Loss\", marker='x')\n",
        "    plt.plot(range(1, epoch+1), val_disc_losses, label=\"Val Disc Loss\", marker='x')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    path = os.path.join(outdir, f\"loss_curve_epoch_{epoch:03d}.png\")\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def save_epoch_image(model, epoch, sample, outdir):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    preds = model(sample, training=False)\n",
        "    plt.figure(figsize=(4,4))\n",
        "\n",
        "    for i in range(preds.shape[0]):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow((preds[i] * 0.5 + 0.5))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    path = os.path.join(outdir, f\"epoch_{epoch:04d}.png\")\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def update_psnr_log(generator, clean_samples, epoch, psnr_values, outdir):\n",
        "    score = evaluate_psnr(generator, clean_samples)\n",
        "    psnr_values.append(score)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, len(psnr_values) + 1), psnr_values, marker=\"o\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"PSNR (dB)\")\n",
        "    plt.title(\"PSNR per Epoch\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    path = os.path.join(outdir, \"psnr_curve.png\")\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "    return psnr_values\n",
        "\n",
        "#I tried to be fancy and write a loop and automate the model tuning\n",
        "#It failed and I can't figure out why, something wrong with Tensorflow\n",
        "#I just put the values I want and it will still run a single iteration for me\n",
        "#before the program crashes\n",
        "\n",
        "lambda_values = [80]\n",
        "adam_values = [1e-4]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for LAMBDA in lambda_values:\n",
        "    for adam_lr in adam_values:\n",
        "        print(f\"\\n\\n=== Training with LAMBDA={LAMBDA}, Adam LR={adam_lr} ===\")\n",
        "\n",
        "        img_dir = f\"{BASE}/lambda{LAMBDA}_adam{adam_lr}\"\n",
        "        generator = make_generator_model()\n",
        "        discriminator = make_discriminator_model()\n",
        "\n",
        "        generator_optimizer = tf.keras.optimizers.Adam(adam_lr)\n",
        "        discriminator_optimizer = tf.keras.optimizers.Adam(adam_lr)\n",
        "\n",
        "        generator_loss_fn = make_generator_loss(LAMBDA)\n",
        "\n",
        "        #Training loop\n",
        "        for epoch in range(1, EPOCHS + 1):\n",
        "            for noisy_batch, clean_batch in dataset:\n",
        "                train_step(noisy_batch, clean_batch,\n",
        "                           generator, discriminator,\n",
        "                           generator_optimizer, discriminator_optimizer,\n",
        "                           generator_loss_fn)\n",
        "\n",
        "            avg_train_gen, avg_train_disc = compute_losses(dataset, generator, discriminator, generator_loss_fn)\n",
        "            train_gen_losses.append(avg_train_gen)\n",
        "            train_disc_losses.append(avg_train_disc)\n",
        "\n",
        "            #Log all outputs and scores\n",
        "            val_dataset = tf.data.Dataset.from_tensor_slices((sample_noisy, sample_clean)).batch(BATCH_SIZE)\n",
        "            avg_val_gen, avg_val_disc = compute_losses(val_dataset, generator, discriminator, generator_loss_fn)\n",
        "            val_gen_losses.append(avg_val_gen)\n",
        "            val_disc_losses.append(avg_val_disc)\n",
        "\n",
        "            save_loss_curve(train_gen_losses, train_disc_losses,\n",
        "                    val_gen_losses, val_disc_losses,\n",
        "                    epoch, img_dir)\n",
        "\n",
        "            print(f\"Epoch {epoch}/{EPOCHS} complete\")\n",
        "\n",
        "            img_dir = f\"{BASE}/lambda{LAMBDA}_adam{adam_lr}\"\n",
        "            save_epoch_image(generator, epoch+1, sample_noisy, img_dir)\n",
        "            psnr_values = update_psnr_log(generator,\n",
        "                                  sample_clean,\n",
        "                                  epoch,\n",
        "                                  psnr_values,\n",
        "                                  img_dir)\n",
        "\n",
        "\n",
        "        #Print directly to console for immediate viewing of results\n",
        "        psnr_score = evaluate_psnr(generator, sample_clean)\n",
        "        print(f\"[RESULT] LAMBDA={LAMBDA}, Adam={adam_lr} â†’ PSNR={psnr_score:.2f}\")\n",
        "\n",
        "\n",
        "        results[(LAMBDA, adam_lr)] = psnr_score\n",
        "\n",
        "        gen_path = f\"{img_dir}/generator.keras\"\n",
        "        disc_path = f\"{img_dir}/discriminator.keras\"\n",
        "\n",
        "        #Save the final model\n",
        "        generator.save(gen_path)\n",
        "        discriminator.save(disc_path)\n",
        "\n",
        "        print(f\"Saved models to {gen_path} and {disc_path}\")\n"
      ]
    }
  ]
}