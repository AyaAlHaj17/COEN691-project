{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsABGkoTDb23BuNeB2ISMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyaAlHaj17/COEN691-project/blob/main/notebooks/ResNet/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeDWhCZjzrFX"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q datasets huggingface_hub pillow torch torchvision tqdm requests\n",
        "!pip install -q scikit-image\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class ImprovedResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.ca = ChannelAttention(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.ca(out)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiScaleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv2d(channels, channels//4, 1)\n",
        "        self.branch2 = nn.Conv2d(channels, channels//4, 3, padding=1)\n",
        "        self.branch3 = nn.Conv2d(channels, channels//4, 5, padding=2)\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, stride=1, padding=1),\n",
        "            nn.Conv2d(channels, channels//4, 1)\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1(x)\n",
        "        b2 = self.branch2(x)\n",
        "        b3 = self.branch3(x)\n",
        "        b4 = self.branch4(x)\n",
        "        out = torch.cat([b1, b2, b3, b4], dim=1)\n",
        "        return self.bn(out)\n",
        "\n",
        "\n",
        "\n",
        "class EnhancedResNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, num_blocks=12, features=64):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.conv_input = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features, 3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.multi_scale = MultiScaleBlock(features)\n",
        "\n",
        "\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ImprovedResidualBlock(features) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.fusion_conv = nn.Conv2d(features * 3, features, 1)\n",
        "\n",
        "\n",
        "        self.conv_output = nn.Sequential(\n",
        "            nn.Conv2d(features, features, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv_input(x)\n",
        "        feat = self.multi_scale(feat)\n",
        "\n",
        "        block_feats = []\n",
        "        for i, block in enumerate(self.res_blocks):\n",
        "            feat = block(feat)\n",
        "            if i in [len(self.res_blocks)//4, len(self.res_blocks)//2, 3*len(self.res_blocks)//4]:\n",
        "                block_feats.append(feat)\n",
        "\n",
        "        if len(block_feats) == 3:\n",
        "            feat = self.fusion_conv(torch.cat(block_feats, dim=1))\n",
        "\n",
        "        out = self.conv_output(feat)\n",
        "        out = x + out\n",
        "\n",
        "        return torch.clamp(out, 0, 1)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "class RobustImageRestorationDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, degradation_type='denoise',\n",
        "                 transform=None, subset_size=None, max_retries=3):\n",
        "        self.dataset = hf_dataset\n",
        "        self.degradation_type = degradation_type\n",
        "        self.max_retries = max_retries\n",
        "        self.failed_count = 0\n",
        "\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "\n",
        "        if subset_size and subset_size < len(hf_dataset):\n",
        "            indices = random.sample(range(len(hf_dataset)), subset_size)\n",
        "            self.indices = indices\n",
        "        else:\n",
        "            self.indices = list(range(len(hf_dataset)))\n",
        "\n",
        "        print(f\"Dataset size: {len(self.indices)} images\")\n",
        "\n",
        "    def _extract_url(self, item):\n",
        "        if 'url' in item:\n",
        "            return item['url']\n",
        "        elif 'image_url' in item:\n",
        "            return item['image_url']\n",
        "        else:\n",
        "            for value in item.values():\n",
        "                if isinstance(value, str) and value.startswith('http'):\n",
        "                    return value\n",
        "        return None\n",
        "\n",
        "    def _add_gaussian_noise(self, image, noise_level=25):\n",
        "        noise = torch.randn_like(image) * (noise_level / 255.0)\n",
        "        return torch.clamp(image + noise, 0, 1)\n",
        "\n",
        "    def _jpeg_compression(self, image, quality=10):\n",
        "        from torchvision.transforms.functional import to_pil_image, to_tensor\n",
        "        pil_img = to_pil_image(image)\n",
        "        buffer = BytesIO()\n",
        "        pil_img.save(buffer, format='JPEG', quality=quality)\n",
        "        buffer.seek(0)\n",
        "        compressed = Image.open(buffer)\n",
        "        return to_tensor(compressed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.indices[idx]\n",
        "\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                item = self.dataset[actual_idx]\n",
        "                url = self._extract_url(item)\n",
        "\n",
        "                if not url:\n",
        "                    raise ValueError(\"No URL found\")\n",
        "\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "                if image.mode != 'RGB':\n",
        "                    image = image.convert('RGB')\n",
        "\n",
        "                clean_image = self.transform(image)\n",
        "\n",
        "\n",
        "                if self.degradation_type == 'denoise':\n",
        "                    degraded_image = self._add_gaussian_noise(clean_image, 25)\n",
        "                elif self.degradation_type == 'dejpeg':\n",
        "                    degraded_image = self._jpeg_compression(clean_image, 10)\n",
        "                else:\n",
        "                    degraded_image = self._add_gaussian_noise(clean_image, 25)\n",
        "\n",
        "                return degraded_image, clean_image\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == self.max_retries - 1:\n",
        "                    self.failed_count += 1\n",
        "\n",
        "                    clean = torch.rand(3, 256, 256)\n",
        "                    degraded = self._add_gaussian_noise(clean, 25)\n",
        "                    return degraded, clean\n",
        "                time.sleep(0.5)\n",
        "\n",
        "\n",
        "        clean = torch.rand(3, 256, 256)\n",
        "        degraded = self._add_gaussian_noise(clean, 25)\n",
        "        return degraded, clean\n",
        "\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.l2_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        l1 = self.l1_loss(pred, target)\n",
        "        l2 = self.l2_loss(pred, target)\n",
        "        return l1 + 0.1 * l2\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "\n",
        "    for degraded, clean in tqdm(dataloader, desc=\"Training\"):\n",
        "        try:\n",
        "            degraded, clean = degraded.to(device), clean.to(device)\n",
        "\n",
        "            # Check for invalid data\n",
        "            if torch.isnan(degraded).any() or torch.isnan(clean).any():\n",
        "                continue\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(degraded)\n",
        "            loss = criterion(output, clean)\n",
        "\n",
        "            # Check for invalid loss\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Batch error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return total_loss / max(valid_batches, 1)\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_psnr = 0\n",
        "    total_ssim = 0\n",
        "    valid_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for degraded, clean in tqdm(dataloader, desc=\"Validation\"):\n",
        "            try:\n",
        "                degraded, clean = degraded.to(device), clean.to(device)\n",
        "\n",
        "                if torch.isnan(degraded).any() or torch.isnan(clean).any():\n",
        "                    continue\n",
        "\n",
        "                output = model(degraded)\n",
        "                loss = criterion(output, clean)\n",
        "\n",
        "                if not torch.isnan(loss) and not torch.isinf(loss):\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "\n",
        "                for i in range(output.size(0)):\n",
        "                    img_pred = output[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                    img_clean = clean[i].cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "                    img_pred = np.clip(img_pred, 0, 1)\n",
        "                    img_clean = np.clip(img_clean, 0, 1)\n",
        "\n",
        "                    try:\n",
        "                        psnr_val = psnr(img_clean, img_pred, data_range=1.0)\n",
        "                        ssim_val = ssim(img_clean, img_pred, data_range=1.0,\n",
        "                                       channel_axis=2, win_size=11)\n",
        "\n",
        "                        if not np.isnan(psnr_val) and not np.isinf(psnr_val):\n",
        "                            total_psnr += psnr_val\n",
        "                            total_ssim += ssim_val\n",
        "                            valid_samples += 1\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    avg_loss = total_loss / max(len(dataloader), 1)\n",
        "    avg_psnr = total_psnr / max(valid_samples, 1)\n",
        "    avg_ssim = total_ssim / max(valid_samples, 1)\n",
        "\n",
        "    return avg_loss, avg_psnr, avg_ssim\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENHANCED ResNet Image Restoration Training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "DEGRADATION_TYPE = 'denoise'\n",
        "BATCH_SIZE = 4 if device.type == 'cpu' else 8\n",
        "NUM_EPOCHS = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "SUBSET_SIZE = 500\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"  Degradation: {DEGRADATION_TYPE}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Dataset Size: {SUBSET_SIZE}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load dataset\n",
        "print(\"\\nLoading HQ-50K dataset...\")\n",
        "try:\n",
        "    dataset = load_dataset(\"YangQiee/HQ-50K\", split=\"train\")\n",
        "    print(f\"Dataset loaded: {len(dataset)} total images\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please check your internet connection and try again.\")\n",
        "    raise\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * min(len(dataset), SUBSET_SIZE * 1.25))\n",
        "val_size = int(0.2 * min(len(dataset), SUBSET_SIZE * 1.25))\n",
        "\n",
        "all_indices = list(range(len(dataset)))\n",
        "random.shuffle(all_indices)\n",
        "train_indices = all_indices[:train_size]\n",
        "val_indices = all_indices[train_size:train_size + val_size]\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = RobustImageRestorationDataset(\n",
        "    dataset.select(train_indices),\n",
        "    degradation_type=DEGRADATION_TYPE,\n",
        "    subset_size=int(SUBSET_SIZE * 0.8)\n",
        ")\n",
        "\n",
        "val_dataset = RobustImageRestorationDataset(\n",
        "    dataset.select(val_indices),\n",
        "    degradation_type=DEGRADATION_TYPE,\n",
        "    subset_size=int(SUBSET_SIZE * 0.2)\n",
        ")\n",
        "\n",
        "# Create dataloaders - NO timeout when num_workers=0\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\nInitializing Enhanced ResNet model...\")\n",
        "model = EnhancedResNet(num_blocks=12, features=64).to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = CombinedLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=NUM_EPOCHS, eta_min=1e-7\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "best_psnr = 0\n",
        "best_ssim = 0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "train_losses, val_losses, val_psnrs, val_ssims = [], [], [], []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "\n",
        "    val_loss, val_psnr, val_ssim = validate(model, val_loader, criterion, device)\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_psnrs.append(val_psnr)\n",
        "    val_ssims.append(val_ssim)\n",
        "\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"LR: {current_lr:.6f}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | PSNR: {val_psnr:.2f} dB | SSIM: {val_ssim:.4f}\")\n",
        "\n",
        "\n",
        "    if val_psnr > best_psnr:\n",
        "        improvement = val_psnr - best_psnr\n",
        "        best_psnr = val_psnr\n",
        "        best_ssim = val_ssim\n",
        "        patience_counter = 0\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'psnr': best_psnr,\n",
        "            'ssim': best_ssim,\n",
        "        }, 'best_enhanced_resnet.pth')\n",
        "        print(f\"✓ Saved best model (PSNR: {best_psnr:.2f} dB, SSIM: {best_ssim:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"⚠ No improvement for {patience_counter}/{patience} epochs\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\ Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Training Completed!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best PSNR: {best_psnr:.2f} dB\")\n",
        "print(f\"Best SSIM: {best_ssim:.4f}\")\n",
        "print(f\"Total Epochs: {len(train_losses)}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
        "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Progress: Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(val_psnrs, color='purple', linewidth=2, marker='o')\n",
        "axes[1].axhline(y=best_psnr, color='r', linestyle='--', alpha=0.5)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('PSNR (dB)')\n",
        "axes[1].set_title('Validation PSNR')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(val_ssims, color='green', linewidth=2, marker='s')\n",
        "axes[2].axhline(y=best_ssim, color='r', linestyle='--', alpha=0.5)\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('SSIM')\n",
        "axes[2].set_title('Validation SSIM')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "checkpoint = torch.load('best_enhanced_resnet.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "\n",
        "num_samples = 4\n",
        "fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
        "\n",
        "print(\"\\nTesting on sample images...\")\n",
        "with torch.no_grad():\n",
        "    for i in range(num_samples):\n",
        "        try:\n",
        "            degraded, clean = val_dataset[i]\n",
        "            degraded_input = degraded.unsqueeze(0).to(device)\n",
        "            restored = model(degraded_input).squeeze(0).cpu()\n",
        "\n",
        "            degraded_np = np.clip(degraded.numpy().transpose(1, 2, 0), 0, 1)\n",
        "            clean_np = np.clip(clean.numpy().transpose(1, 2, 0), 0, 1)\n",
        "            restored_np = np.clip(restored.numpy().transpose(1, 2, 0), 0, 1)\n",
        "\n",
        "            psnr_degraded = psnr(clean_np, degraded_np, data_range=1.0)\n",
        "            psnr_restored = psnr(clean_np, restored_np, data_range=1.0)\n",
        "            ssim_degraded = ssim(clean_np, degraded_np, data_range=1.0, channel_axis=2)\n",
        "            ssim_restored = ssim(clean_np, restored_np, data_range=1.0, channel_axis=2)\n",
        "\n",
        "            axes[i, 0].imshow(degraded_np)\n",
        "            axes[i, 0].set_title(f'Degraded\\nPSNR: {psnr_degraded:.2f} dB | SSIM: {ssim_degraded:.3f}')\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            axes[i, 1].imshow(restored_np)\n",
        "            axes[i, 1].set_title(f'Restored\\nPSNR: {psnr_restored:.2f} dB (+{psnr_restored-psnr_degraded:.2f}) | SSIM: {ssim_restored:.3f}',\n",
        "                                color='green', fontweight='bold')\n",
        "            axes[i, 1].axis('off')\n",
        "\n",
        "            axes[i, 2].imshow(clean_np)\n",
        "            axes[i, 2].set_title('Ground Truth')\n",
        "            axes[i, 2].axis('off')\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"=\"*70)\n",
        "print(f\"✓ Best model saved: best_enhanced_resnet.pth\")\n",
        "print(f\"✓ Training curves saved: training_curves.png\")\n",
        "print(f\"✓ Test results saved: test_results.png\")\n",
        "print(f\"✓ Final PSNR: {best_psnr:.2f} dB\")\n",
        "print(f\"✓ Final SSIM: {best_ssim:.4f}\")\n",
        "print(\"=\"*70)"
      ]
    }
  ]
}